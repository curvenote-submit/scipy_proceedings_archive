
@MISC{Min_undated-qd,
  title        = "Thebelab",
  author       = "Min, R K",
  url          = "https://github.com/minrk/thebelab",
  howpublished = "\url{https://github.com/minrk/thebelab}",
  note         = "Accessed: 2018-6-13"
}

@TECHREPORT{The_American_Society_for_Cell_Biology_undated-yv,
  title  = "{ASCB} Member Survey on Reproducibility",
  author = "{The American Society for Cell Biology}",
  url    = "http://www.ascb.org/wp-content/uploads/2015/11/final-survey-results-without-Q11.pdf"
}

@ARTICLE{Perez2007-jc,
  title     = "{IPython}: A System for Interactive Scientific Computing",
  author    = "Perez, F and Granger, B E",
  abstract  = "Python offers basic facilities for interactive work and a
               comprehensive library on top of which more sophisticated systems
               can be built. The IPython project provides on enhanced
               interactive environment that includes, among other features,
               support for data visualization and facilities for distributed
               and parallel computation",
  journal   = "Computing in Science Engineering",
  publisher = "ieeexplore.ieee.org",
  volume    =  9,
  number    =  3,
  pages     = "21--29",
  month     =  may,
  year      =  2007,
  url       = "http://dx.doi.org/10.1109/MCSE.2007.53",
  keywords  = "data visualisation;natural sciences computing;object-oriented
               languages;object-oriented programming;parallel
               programming;software libraries;IPython;comprehensive
               library;data visualization;distributed computation;interactive
               scientific computing;parallel computation;Data analysis;Data
               visualization;Hardware;Libraries;Parallel
               processing;Production;Scientific
               computing;Spine;Supercomputers;Testing;Python;computer
               languages;scientific computing;scientific programming"
}

@MISC{Pineau2017-ya,
  title        = "{ICLR} 2018 Reproducibility Challenge",
  author       = "Pineau, Joelle and Fried, Genevieve and Ke, Rosemary Nan and
                  Larochelle, Hugo",
  year         =  2017,
  url          = "https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html",
  howpublished = "\url{https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html}",
  note         = "Accessed: 2018-6-10"
}

@ARTICLE{Stodden2018-bo,
  title     = "An empirical analysis of journal policy effectiveness for
               computational reproducibility",
  author    = "Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun",
  abstract  = "A key component of scientific communication is sufficient
               information for other researchers in the field to reproduce
               published findings. For computational and data-enabled research,
               this has often been interpreted to mean making available the raw
               data from which results were generated, the computer code that
               generated the findings, and any additional information needed
               such as workflows and input parameters. Many journals are
               revising author guidelines to include data and code
               availability. This work evaluates the effectiveness of journal
               policy that requires the data and code necessary for
               reproducibility be made available postpublication by the authors
               upon request. We assess the effectiveness of such a policy by
               (i) requesting data and code from authors and (ii) attempting
               replication of the published findings. We chose a random sample
               of 204 scientific papers published in the journal Science after
               the implementation of their policy in February 2011. We found
               that we were able to obtain artifacts from 44\% of our sample
               and were able to reproduce the findings for 26\%. We find this
               policy---author remission of data and code postpublication upon
               request---an improvement over no policy, but currently
               insufficient for reproducibility.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Academy of Sciences",
  pages     = "201708290",
  month     =  mar,
  year      =  2018,
  url       = "http://www.pnas.org/content/early/2018/03/08/1708290115.short",
  language  = "en"
}

@ARTICLE{Stodden2013-sf,
  title     = "Toward Reproducible Computational Research: An Empirical
               Analysis of Data and Code Policy Adoption by Journals",
  author    = "Stodden, Victoria and Guo, Peixuan and Ma, Zhaokun",
  abstract  = "Journal policy on research data and code availability is an
               important part of the ongoing shift toward publishing
               reproducible computational science. This article extends the
               literature by studying journal data sharing policies by year
               (for both 2011 and 2012) for a referent set of 170 journals. We
               make a further contribution by evaluating code sharing policies,
               supplemental materials policies, and open access status for
               these 170 journals for each of 2011 and 2012. We build a
               predictive model of open data and code policy adoption as a
               function of impact factor and publisher and find higher impact
               journals more likely to have open data and code policies and
               scientific societies more likely to have open data and code
               policies than commercial publishers. We also find open data
               policies tend to lead open code policies, and we find no
               relationship between open data and code policies and either
               supplemental material policies or open access journal status. Of
               the journals in this study, 38\% had a data policy, 22\% had a
               code policy, and 66\% had a supplemental materials policy as of
               June 2012. This reflects a striking one year increase of 16\% in
               the number of data policies, a 30\% increase in code policies,
               and a 7\% increase in the number of supplemental materials
               policies. We introduce a new dataset to the community that
               categorizes data and code sharing, supplemental materials, and
               open access policies in 2011 and 2012 for these 170 journals.",
  journal   = "PLoS One",
  publisher = "Public Library of Science",
  volume    =  8,
  number    =  6,
  pages     = "e67111",
  month     =  jun,
  year      =  2013,
  url       = "http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0067111&type=printable"
}

@ARTICLE{Baker2016-gp,
  title    = "1,500 scientists lift the lid on reproducibility",
  author   = "Baker, Monya",
  journal  = "Nature",
  volume   =  533,
  number   =  7604,
  pages    = "452--454",
  month    =  may,
  year     =  2016,
  url      = "http://dx.doi.org/10.1038/533452a",
  language = "en"
}

@INPROCEEDINGS{Zhang_undated-mn,
  title       = "Reproducible Environments for Reproducible Results",
  booktitle   = "{SciPy} 2018",
  author      = "Zhang, Bihan and Macdonald, Austin",
  abstract    = "GitHub is where people build software. More than 28 million
                 people use GitHub to discover, fork, and contribute to over 85
                 million projects.",
  institution = "Github",
  url         = "https://github.com/werwty/scipy_proceedings"
}

@INPROCEEDINGS{Gundersen_undated-jw,
  title     = "State of the Art: Reproducibility in Artificial Intelligence",
  booktitle = "{Thirty-Second} {AAAI} Conference on Artificial Intelligence",
  author    = "Gundersen, Odd Erik and Kjensmo, Sigbj{\o}rn",
  url       = "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17248/15864"
}

@MISC{noauthor_undated-ng,
  title       = "aaai2018-paperid-62",
  abstract    = "GitHub is where people build software. More than 28 million
                 people use GitHub to discover, fork, and contribute to over 85
                 million projects.",
  institution = "Github",
  url         = "https://github.com/aaai2018-paperid-62/aaai2018-paperid-62"
}

@ARTICLE{Hutson2018-as,
  title    = "Artificial intelligence faces reproducibility crisis",
  author   = "Hutson, Matthew",
  journal  = "Science",
  volume   =  359,
  number   =  6377,
  pages    = "725--726",
  month    =  feb,
  year     =  2018,
  url      = "http://dx.doi.org/10.1126/science.359.6377.725",
  language = "en"
}

@ARTICLE{Alberti2018-bf,
  title         = "{DeepDIVA}: A {Highly-Functional} Python Framework for
                   Reproducible Experiments",
  author        = "Alberti, Michele and Pondenkandath, Vinaychandran and
                   W{\"u}rsch, Marcel and Ingold, Rolf and Liwicki, Marcus",
  abstract      = "We introduce DeepDIVA: an infrastructure designed to enable
                   quick and intuitive setup of reproducible experiments with a
                   large range of useful analysis functionality. Reproducing
                   scientific results can be a frustrating experience, not only
                   in document image analysis but in machine learning in
                   general. Using DeepDIVA a researcher can either reproduce a
                   given experiment with a very limited amount of information
                   or share their own experiments with others. Moreover, the
                   framework offers a large range of functions, such as
                   boilerplate code, keeping track of experiments,
                   hyper-parameter optimization, and visualization of data and
                   results. To demonstrate the effectiveness of this framework,
                   this paper presents case studies in the area of handwritten
                   document analysis where researchers benefit from the
                   integrated functionality. DeepDIVA is implemented in Python
                   and uses the deep learning framework PyTorch. It is
                   completely open source, and accessible as Web Service
                   through DIVAServices.",
  month         =  apr,
  year          =  2018,
  url           = "http://arxiv.org/abs/1805.00329",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1805.00329"
}

@MISC{noauthor_2018-nu,
  title        = "Missing data hinder replication of artificial intelligence
                  studies",
  booktitle    = "Science | {AAAS}",
  abstract     = "Unpublished code and sensitive training conditions aggravate
                  reproducibility crisis in computer science",
  month        =  feb,
  year         =  2018,
  url          = "http://www.sciencemag.org/news/2018/02/missing-data-hinder-replication-artificial-intelligence-studies",
  howpublished = "\url{http://www.sciencemag.org/news/2018/02/missing-data-hinder-replication-artificial-intelligence-studies}",
  note         = "Accessed: 2018-6-4"
}

@ARTICLE{Cornish2018-mo,
  title         = "The construction and use of {LISA} sensitivity curves",
  author        = "Cornish, Neil and Robson, Travis",
  abstract      = "The goal of this document is to describe the construction
                   and use of sensitivity curves for the Laser Interferometer
                   Space Antenna. Design parameters from the current 2018
                   Phase-0 reference design are used.",
  month         =  mar,
  year          =  2018,
  url           = "http://arxiv.org/abs/1803.01944",
  archivePrefix = "arXiv",
  primaryClass  = "astro-ph.HE",
  eprint        = "1803.01944"
}

@ARTICLE{Holdgraf2017-so,
  title    = "Encoding and Decoding Models in Cognitive Electrophysiology",
  author   = "Holdgraf, Christopher R and Rieger, Jochem W and Micheli,
              Cristiano and Martin, Stephanie and Knight, Robert T and
              Theunissen, Frederic E",
  abstract = "Cognitive neuroscience has seen rapid growth in the size and
              complexity of data recorded from the human brain as well as in
              the computational tools available to analyze this data. This data
              explosion has resulted in an increased use of multivariate,
              model-based methods for asking neuroscience questions, allowing
              scientists to investigate multiple hypotheses with a single
              dataset, to use complex, time-varying stimuli, and to study the
              human brain under more naturalistic conditions. These tools come
              in the form of ``Encoding'' models, in which stimulus features
              are used to model brain activity, and ``Decoding'' models, in
              which neural features are used to generated a stimulus output.
              Here we review the current state of encoding and decoding models
              in cognitive electrophysiology and provide a practical guide
              toward conducting experiments and analyses in this emerging
              field. Our examples focus on using linear models in the study of
              human language and audition. We show how to calculate auditory
              receptive fields from natural sounds as well as how to decode
              neural recordings to predict speech. The paper aims to be a
              useful tutorial to these approaches, and a practical introduction
              to using machine learning and applied statistics to build models
              of neural activity. The data analytic approaches we discuss may
              also be applied to other sensory modalities, motor systems, and
              cognitive systems, and we cover some examples in these areas. In
              addition, a collection of Jupyter notebooks is publicly available
              as a complement to the material covered in this paper, providing
              code examples and tutorials for predictive modeling in python.
              The aim is to provide a practical understanding of predictive
              modeling of human brain data and to propose best-practices in
              conducting these analyses.",
  journal  = "Front. Syst. Neurosci.",
  volume   =  11,
  pages    = "61",
  month    =  sep,
  year     =  2017,
  url      = "http://dx.doi.org/10.3389/fnsys.2017.00061",
  keywords = "decoding models; electrocorticography (ECoG);
              electrophysiology/evoked potentials; encoding models; machine
              learning applied to neuroscience; natural stimuli; predictive
              modeling; tutorials",
  language = "en"
}

@ARTICLE{Rein2016-rd,
  title     = "Second-order variational equations for N-body simulations",
  author    = "Rein, Hanno and Tamayo, Daniel",
  abstract  = "Abstract. First-order variational equations are widely used in
               N-body simulations to study how nearby trajectories diverge from
               one another. These allow for ef",
  journal   = "Monthly Notices of the Royal Astronomical Society",
  publisher = "Oxford University Press",
  volume    =  459,
  number    =  3,
  pages     = "2275--2285",
  month     =  jul,
  year      =  2016,
  url       = "https://academic.oup.com/mnras/article/459/3/2275/2595117"
}

@ARTICLE{Neyrinck2018-xy,
  title     = "The cosmic spiderweb: equivalence of cosmic, architectural and
               origami tessellations",
  author    = "Neyrinck, Mark C and Hidding, Johan and Konstantatou, Marina and
               van de Weygaert, Rien",
  abstract  = "For over 20 years, the term `cosmic web' has guided our
               understanding of the large-scale arrangement of matter in the
               cosmos, accurately evoking the concept of a network of galaxies
               linked by filaments. But the physical correspondence between the
               cosmic web and structural engineering or textile `spiderwebs' is
               even deeper than previously known, and also extends to origami
               tessellations. Here, we explain that in a good
               structure-formation approximation known as the adhesion model,
               threads of the cosmic web form a spiderweb, i.e. can be strung
               up to be entirely in tension. The correspondence is exact if
               nodes sampling voids are included, and if structure is excluded
               within collapsed regions (walls, filaments and haloes), where
               dark-matter multistreaming and baryonic physics affect the
               structure. We also suggest how concepts arising from this link
               might be used to test cosmological models: for example, to test
               for large-scale anisotropy and rotational flows in the cosmos.",
  journal   = "Royal Society Open Science",
  publisher = "The Royal Society",
  volume    =  5,
  number    =  4,
  pages     = "171582",
  month     =  apr,
  year      =  2018,
  url       = "http://rsos.royalsocietypublishing.org/content/5/4/171582",
  language  = "en"
}

@MISC{JupyterHub2018-ek,
  title       = "binder-billing",
  author      = "{JupyterHub}",
  abstract    = "GitHub is where people build software. More than 27 million
                 people use GitHub to discover, fork, and contribute to over 80
                 million projects.",
  institution = "Github",
  year        =  2018,
  url         = "https://github.com/jupyterhub/binder-billing"
}

@MISC{Atlassian_undated-ra,
  title        = "Bitbucket",
  booktitle    = "Bitbucket",
  author       = "{Atlassian}",
  abstract     = "Collaborate on code with inline comments and pull requests.
                  Manage and share your Git repositories to build and ship
                  software, as a team.",
  url          = "https://bitbucket.org",
  howpublished = "\url{https://bitbucket.org}",
  note         = "Accessed: 2018-5-24"
}

@MISC{GitHub_undated-wa,
  title       = "{GitHub}",
  author      = "{GitHub}",
  abstract    = "GitHub is where people build software. More than 27 million
                 people use GitHub to discover, fork, and contribute to over 80
                 million projects.",
  institution = "Github",
  url         = "https://github.com"
}

@MISC{Docker_Inc_undated-ai,
  title        = "Docker",
  booktitle    = "Docker",
  author       = "{Docker, Inc.}",
  abstract     = "Docker is an open platform for developers and sysadmins to
                  build, ship, and run distributed applications, whether on
                  laptops, data center VMs, or the cloud.",
  url          = "https://www.docker.com/",
  howpublished = "\url{https://www.docker.com/}",
  note         = "Accessed: 2018-5-24"
}

@MISC{Open_Humans_Foundation_undated-ov,
  title        = "Personal Data Notebooks",
  booktitle    = "Open Humans",
  author       = "{Open Humans Foundation}",
  url          = "https://www.openhumans.org/activity/personal-data-notebooks/",
  howpublished = "\url{https://www.openhumans.org/activity/personal-data-notebooks/}",
  note         = "Accessed: 2018-5-24"
}

@MISC{Head2018-jf,
  title       = "openrefineder",
  author      = "Head, Tim",
  abstract    = "GitHub is where people build software. More than 27 million
                 people use GitHub to discover, fork, and contribute to over 80
                 million projects.",
  institution = "Github",
  year        =  2018,
  url         = "https://github.com/betatim/openrefineder"
}

@MISC{RK_Min2018-eq,
  title       = "nbstencilaproxy",
  author      = "{RK, Min} and N{\"u}st, Daniel",
  abstract    = "GitHub is where people build software. More than 27 million
                 people use GitHub to discover, fork, and contribute to over 80
                 million projects.",
  institution = "Github",
  year        =  2018,
  url         = "https://github.com/minrk/nbstencilaproxy"
}

@MISC{Project_Juptyer_Contributors2017-ra,
  title    = "Using {R} with Jupyter / {RStudio} on Binder",
  author   = "{Project Juptyer Contributors}",
  year     =  2017,
  url      = "https://github.com/binder-examples/r"
}

@MISC{Project_Jupyter_Contributors2017-yi,
  title    = "jupyterlab-demo",
  author   = "{Project Jupyter Contributors}",
  year     =  2017,
  url      = "https://github.com/jupyterlab/jupyterlab-demo"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Bussonnier2018-kc,
  title        = "{I} Python, You R, We Julia",
  booktitle    = "Medium",
  author       = "Bussonnier, Matthias",
  abstract     = "When we decided to rename part of the IPython project to
                  Jupyter in 2014, we had many good reasons. Our goal was to
                  make (Data)Science and Education better, by providing Free
                  and Open-Source tools…",
  publisher    = "Medium",
  month        =  apr,
  year         =  2018,
  url          = "https://medium.com/@mbussonn/baf064ca1fb6",
  howpublished = "\url{https://medium.com/@mbussonn/baf064ca1fb6}",
  note         = "Accessed: 2018-5-23"
}

@MISC{Cloud_Native_Computing_Foundation_undated-fl,
  title    = "Kubernetes",
  author   = "{Cloud Native Computing Foundation}",
  url      = "https://kubernetes.io/"
}

@MISC{Berkeley_Division_of_Data_Sciences_undated-nz,
  title        = "Foundations of Data Science",
  booktitle    = "data8",
  author       = "{Berkeley Division of Data Sciences}",
  url          = "http://data8.org/",
  howpublished = "\url{http://data8.org/}",
  note         = "Accessed: 2018-5-23"
}

@MISC{Wikimedia_undated-si,
  title        = "{PAWS}: A Web Shell",
  author       = "{Wikimedia}",
  url          = "https://wikitech.wikimedia.org/wiki/PAWS",
  howpublished = "\url{https://wikitech.wikimedia.org/wiki/PAWS}",
  note         = "Accessed: 2018-5-23"
}

@MISC{Microsoft_undated-gd,
  title    = "Microsoft {R} Application Network",
  author   = "{Microsoft}",
  url      = "https://mran.microsoft.com/"
}

@MISC{Nteract_contributors2016-dg,
  title    = "nteract",
  author   = "{nteract contributors}",
  abstract = "nteract is a desktop application that allows you to develop rich
              documents that contain prose, executable code, and images.",
  year     =  2016,
  url      = "https://play.nteract.io/"
}

@MISC{noauthor_undated-ux,
  title        = "Binder Grafana board",
  booktitle    = "Grafana",
  url          = "https://grafana.mybinder.org/?orgId=1",
  howpublished = "\url{https://grafana.mybinder.org/?orgId=1}",
  note         = "Accessed: 2018-5-23"
}

@INCOLLECTION{Adhikari_undated-jh,
  title     = "Plotting the Classics",
  booktitle = "Computational and Inferential Thinking",
  author    = "Adhikari, Ani and Denero, John",
  editor    = "Adhikari, Ani and Denero, John",
  url       = "https://www.inferentialthinking.com/chapters/01/3/plotting-the-classics.html"
}

@MISC{Najera_undated-uy,
  title        = "Configuration --- {Sphinx-Gallery} 0.1.13-git documentation",
  booktitle    = "{Sphinx-Gallery}",
  author       = "N{\'a}jera, {\'O}scar",
  url          = "https://sphinx-gallery.readthedocs.io/en/latest/advanced_configuration.html#binder-links",
  howpublished = "\url{https://sphinx-gallery.readthedocs.io/en/latest/advanced_configuration.html#binder-links}",
  note         = "Accessed: 2018-5-23"
}

@BOOK{Downey2016-hx,
  title     = "Think {DSP}: Digital Signal Processing in Python",
  author    = "Downey, Allen B",
  publisher = "O'Reilly Media",
  edition   = "1 edition",
  month     =  aug,
  year      =  2016,
  url       = "https://www.amazon.com/gp/product/1491938455/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1491938455&linkCode=as2&tag=greenteapre01-20&linkId=CTV7PDT7E5EGGJUM",
  language  = "en"
}

@MISC{GESIS_Leibniz_Institute_for_the_Social_Sciences_undated-sn,
  title        = "{GESIS} Notebooks (beta)",
  author       = "{GESIS -- Leibniz Institute for the Social Sciences}",
  abstract     = "GESIS Notebooks (beta)",
  url          = "https://notebooks.gesis.org/",
  howpublished = "\url{https://notebooks.gesis.org/}",
  note         = "Accessed: 2018-5-23"
}

@ARTICLE{V_Stodden_D_H_Bailey_J_Borwein_R_J_LeVeque_W_Rider_and_W_Stein2012-ox,
  title   = "Setting the Default to Reproducible",
  author  = "{V. Stodden, D. H. Bailey, J. Borwein, R. J. LeVeque, W. Rider,
             and W. Stein}",
  journal = "Reproducibility in Computational and Experimental Mathematics",
  year    =  2012,
  url     = "https://www.carma.newcastle.edu.au/jon/icerm12.pdf"
}

@ARTICLE{Somers2018-bj,
  title    = "The Scientific Paper Is Obsolete",
  author   = "Somers, James",
  abstract = "Here's what's next.",
  journal  = "The Atlantic",
  month    =  apr,
  year     =  2018,
  url      = "https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/"
}

@ARTICLE{Gentleman2007-cz,
  title     = "Statistical Analyses and Reproducible Research",
  author    = "Gentleman, Robert and Lang, Duncan Temple",
  abstract  = "It is important, if not essential, to integrate the computations
               and code used in data analyses, methodological descriptions,
               simulations, and so on with the documents that describe and rely
               on them. This integration allows readers to both verify and
               adapt the claims in the documents. Authors can easily reproduce
               the results in the future, and they can present the document's
               contents in a different medium, for example, with interactive
               controls. This article describes a software framework for both
               authoring and distributing these integrated, dynamic documents
               that contain text, code, data, and any auxiliary content needed
               to recreate the computations. The documents are dynamic in that
               the contents---including figures, tables, and so on---can be
               recalculated each time a view of the document is generated. Our
               model treats a dynamic document as a master or ``source''
               document from which one can generate different views in the form
               of traditional, derived documents for different audiences. We
               introduce the concept of a compendium as a container for one or
               more dynamic documents and the different elements needed when
               processing them, such as code and data. The compendium serves as
               a means for distributing, managing, and updating the collection.
               The step from disseminating analyses via a compendium to
               reproducible research is a small one. By reproducible research,
               we mean research papers with accompanying software tools that
               allow the reader to directly reproduce the results and employ
               the computational methods that are presented in the research
               paper. Some of the issues involved in paradigms for the
               production, distribution, and use of such reproducible research
               are discussed.",
  journal   = "J. Comput. Graph. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.,
               Institute of Mathematical Statistics, Interface Foundation of
               America]",
  volume    =  16,
  number    =  1,
  pages     = "1--23",
  year      =  2007,
  url       = "http://www.jstor.org/stable/27594227"
}

@ARTICLE{Collberg2014-hh,
  title   = "Measuring reproducibility in computer systems research",
  author  = "Collberg, Christian and Proebsting, Todd and Moraila, Gina and
             Shankaran, Akash and Shi, Zuoming and Warren, Alex M",
  journal = "Department of Computer Science, University of Arizona, Tech. Rep",
  volume  =  37,
  year    =  2014,
  url     = "http://reproducibility.cs.arizona.edu/tr.pdf"
}

@MISC{Stodden_undated-fd,
  title        = "2014 : {WHAT} {SCIENTIFIC} {IDEA} {IS} {READY} {FOR}
                  {RETIREMENT}?",
  booktitle    = "Edge",
  author       = "Stodden, Victoria",
  url          = "https://www.edge.org/response-detail/25340",
  howpublished = "\url{https://www.edge.org/response-detail/25340}",
  note         = "Accessed: 2018-5-23"
}

@ARTICLE{Stodden2018-fy,
  title    = "An empirical analysis of journal policy effectiveness for
              computational reproducibility",
  author   = "Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun",
  abstract = "A key component of scientific communication is sufficient
              information for other researchers in the field to reproduce
              published findings. For computational and data-enabled research,
              this has often been interpreted to mean making available the raw
              data from which results were generated, the computer code that
              generated the findings, and any additional information needed
              such as workflows and input parameters. Many journals are
              revising author guidelines to include data and code availability.
              This work evaluates the effectiveness of journal policy that
              requires the data and code necessary for reproducibility be made
              available postpublication by the authors upon request. We assess
              the effectiveness of such a policy by (i) requesting data and
              code from authors and (ii) attempting replication of the
              published findings. We chose a random sample of 204 scientific
              papers published in the journal Science after the implementation
              of their policy in February 2011. We found that we were able to
              obtain artifacts from 44\% of our sample and were able to
              reproduce the findings for 26\%. We find this policy-author
              remission of data and code postpublication upon request-an
              improvement over no policy, but currently insufficient for
              reproducibility.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  115,
  number   =  11,
  pages    = "2584--2589",
  month    =  mar,
  year     =  2018,
  url      = "http://dx.doi.org/10.1073/pnas.1708290115",
  keywords = "code access; data access; open science; reproducibility policy;
              reproducible research",
  language = "en"
}

@INPROCEEDINGS{Stodden2012-sd,
  title     = "{RunMyCode.org}: A novel dissemination and collaboration
               platform for executing published computational results",
  booktitle = "2012 {IEEE} 8th International Conference on {E-Science}",
  author    = "Stodden, V and Hurlin, C and P{\'e}rignon, C",
  abstract  = "We believe computational science as practiced today suffers from
               a growing credibility gap - it is impossible to replicate most
               of the computational results presented at conferences or
               published in papers today. We argue that this crisis can be
               addressed by the open availability of the code and data that
               generated the results, in other words practicing reproducible
               computational science. In this paper we present a new
               computational infrastructure called RunMyCode.org that is
               designed to support published articles by providing a
               dissemination platform for the code and data that generated the
               their results. Published articles are given a companion webpage
               on the RunMyCode.org website from which a visitor can both
               download the associated code and data, and execute the code in
               the cloud directly through the RunMyCode.org website. This
               permits results to be verified through the companion webpage or
               on a user's local system. RunMyCode.org also permits a user to
               upload their own data to the companion webpage to check the code
               by running it on novel datasets. Through the creation of ``coder
               pages'' for each contributor to RunMyCode.org, we seek to
               facilitate social network-like interaction. Descriptive
               information appears on each coder page, including demographic
               data and other companion pages to which they made contributions.
               In this paper we motivate the rationale and functionality of
               RunMyCode.org and outline a vision of its future.",
  pages     = "1--8",
  month     =  oct,
  year      =  2012,
  url       = "http://dx.doi.org/10.1109/eScience.2012.6404455",
  keywords  = "groupware;information dissemination;natural sciences
               computing;social networking (online);RunMyCode.org Website;coder
               pages;collaboration platform;companion webpage;computational
               science;credibility gap;demographic data;dissemination
               platform;published computational results;social network-like
               interaction;Collaboration;Computers;Economics;Educational
               institutions;Mathematics;Scientific computing;Software;cloud
               computing;code sharing;collaborative networks;data
               sharing;dissemination platform;executable papers;open
               science;reproducible computational science;reproducible research"
}

@ARTICLE{Anjos2017-vb,
  title         = "{BEAT}: An {Open-Source} {Web-Based} {Open-Science} Platform",
  author        = "Anjos, Andr{\'e} and El-Shafey, Laurent and Marcel,
                   S{\'e}bastien",
  abstract      = "With the increased interest in computational sciences,
                   machine learning (ML), pattern recognition (PR) and big
                   data, governmental agencies, academia and manufacturers are
                   overwhelmed by the constant influx of new algorithms and
                   techniques promising improved performance, generalization
                   and robustness. Sadly, result reproducibility is often an
                   overlooked feature accompanying original research
                   publications, competitions and benchmark evaluations. The
                   main reasons behind such a gap arise from natural
                   complications in research and development in this area: the
                   distribution of data may be a sensitive issue; software
                   frameworks are difficult to install and maintain; Test
                   protocols may involve a potentially large set of intricate
                   steps which are difficult to handle. Given the raising
                   complexity of research challenges and the constant increase
                   in data volume, the conditions for achieving reproducible
                   research in the domain are also increasingly difficult to
                   meet. To bridge this gap, we built an open platform for
                   research in computational sciences related to pattern
                   recognition and machine learning, to help on the
                   development, reproducibility and certification of results
                   obtained in the field. By making use of such a system,
                   academic, governmental or industrial organizations enable
                   users to easily and socially develop processing toolchains,
                   re-use data, algorithms, workflows and compare results from
                   distinct algorithms and/or parameterizations with minimal
                   effort. This article presents such a platform and discusses
                   some of its key features, uses and limitations. We overview
                   a currently operational prototype and provide design
                   insights.",
  month         =  apr,
  year          =  2017,
  url           = "http://arxiv.org/abs/1704.02319",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SE",
  eprint        = "1704.02319"
}

@INCOLLECTION{Buckheit1995-ox,
  title     = "{WaveLab} and Reproducible Research",
  booktitle = "Wavelets and Statistics",
  author    = "Buckheit, Jonathan B and Donoho, David L",
  editor    = "Antoniadis, Anestis and Oppenheim, Georges",
  abstract  = "Wavelab is a library of wavelet-packet analysis, cosine-packet
               analysis and matching pursuit. The library is available free of
               charge over the Internet. Versions are provided for Macintosh,
               UNIX and Windows machines.",
  publisher = "Springer New York",
  pages     = "55--81",
  year      =  1995,
  url       = "https://doi.org/10.1007/978-1-4612-2544-7_5",
  address   = "New York, NY"
}

@MISC{Project_Jupyter_Contributors2017-dn,
  title       = "example-conda-environment",
  author      = "{Project Jupyter Contributors}",
  abstract    = "example-conda-environment - An example Binder repository that
                 contains an environment.yml file",
  institution = "Github",
  year        =  2017,
  url         = "https://github.com/binder-project/example-conda-environment"
}

@MISC{Project_Jupyter_Contributors2017-no,
  title       = "repo2docker",
  author      = "{Project Jupyter Contributors}",
  institution = "Github",
  year        =  2017,
  url         = "https://github.com/jupyter/repo2docker/"
}

@MISC{LIGO_Scientific_Collaboration_undated-xy,
  title        = "{LIGO} Open Science Center",
  booktitle    = "{LIGO}",
  author       = "{LIGO Scientific Collaboration}",
  url          = "https://losc.ligo.org/tutorials/",
  howpublished = "\url{https://losc.ligo.org/tutorials/}",
  note         = "Accessed: 2017-12-12"
}

@ARTICLE{Doshi-Velez2017-ax,
  title         = "Towards A Rigorous Science of Interpretable Machine Learning",
  author        = "Doshi-Velez, Finale and Kim, Been",
  abstract      = "As machine learning systems become ubiquitous, there has
                   been a surge of interest in interpretable machine learning:
                   systems that provide explanation for their outputs. These
                   explanations are often used to qualitatively assess other
                   criteria such as safety or non-discrimination. However,
                   despite the interest in interpretability, there is very
                   little consensus on what interpretable machine learning is
                   and how it should be measured. In this position paper, we
                   first define interpretability and describe when
                   interpretability is needed (and when it is not). Next, we
                   suggest a taxonomy for rigorous evaluation and expose open
                   questions towards a more rigorous science of interpretable
                   machine learning.",
  month         =  feb,
  year          =  2017,
  url           = "http://arxiv.org/abs/1702.08608",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1702.08608"
}

@MISC{Project_Jupyter_Contributors_undated-zb,
  title       = "Juptyer Notebook",
  author      = "{Project Jupyter Contributors}",
  abstract    = "notebook - Jupyter Interactive Notebook",
  institution = "Github",
  url         = "https://github.com/jupyter/notebook"
}

@MANUAL{Project_Jupyter_Contributors_undated-sp,
  title        = "Jupyter kernels",
  author       = "{Project Jupyter Contributors}",
  abstract     = "jupyter - Jupyter metapackage for installation, docs and chat",
  url          = "https://github.com/jupyter/jupyter/wiki/Jupyter-kernels",
  organization = "Project Jupyter"
}

@INPROCEEDINGS{Ross2017-ff,
  title           = "Right for the Right Reasons: Training Differentiable
                     Models by Constraining their Explanations",
  booktitle       = "Proceedings of the {Twenty-Sixth} International Joint
                     Conference on Artificial Intelligence",
  author          = "Ross, Andrew Slavin and Hughes, Michael C and Doshi-Velez,
                     Finale",
  abstract        = "Neural networks are among the most accurate supervised
                     learning methods in use today, but their opacity makes
                     them difficult to trust in critical applications,
                     especially when conditions in training differ from those
                     in test. Recent work on explanations for black-box models
                     has produced tools (e.g. LIME) to show the implicit rules
                     behind predictions, which can help us identify when models
                     are right for the wrong reasons. However, these methods do
                     not scale to explaining entire datasets and cannot correct
                     the problems they reveal. We introduce a method for
                     efficiently explaining and regularizing differentiable
                     models by examining and selectively penalizing their input
                     gradients, which provide a normal to the decision
                     boundary. We apply these penalties both based on expert
                     annotation and in an unsupervised fashion that encourages
                     diverse models with qualitatively different decision
                     boundaries for the same classification problem. On
                     multiple datasets, we show our approach generates faithful
                     explanations and models that generalize much better when
                     conditions differ between training and test.",
  pages           = "Pages 2662--2670.",
  month           =  mar,
  year            =  2017,
  url             = "https://www.ijcai.org/proceedings/2017/371",
  conference      = "International Joint Conference on Artificial Intelligence"
}

@MISC{Liang2015-ay,
  title      = "{CodaLab} Worksheets for Reproducible, Executable Papers",
  author     = "Liang, Percy and Viegas, Evelyne",
  abstract   = "We are interested in solving two infrastructural problems in
                data-centric fields such as machine learning: First, an
                inordinate amount of time is spent on preprocessing datasets,
                getting other people's code to run, writing
                evaluation/visualization scripts, with much of this effort
                duplicated across different research groups. Second, a only
                static set of final results are ever published, leaving it up
                to the reader to guess how the various methods would fare in
                unreported scenarios. We present CodaLab Worksheets, a new
                platform which aims to tackle these two problems by creating an
                online community around sharing and executing immutable
                components called bundles, thereby streamlining the research
                process.",
  month      =  dec,
  year       =  2015,
  url        = "https://nips.cc/Conferences/2015/Schedule?showEvent=5779",
  conference = "NIPS 2015, Demonstrations Track"
}

@ARTICLE{Erin_D_Foster2017-tk,
  title     = "Open Science Framework ({OSF})",
  author    = "Erin D. Foster, Ariel Deardorff",
  journal   = "J. Med. Libr. Assoc.",
  publisher = "Medical Library Association",
  volume    =  105,
  number    =  2,
  pages     = "203",
  month     =  apr,
  year      =  2017,
  url       = "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5370619/",
  language  = "en"
}

@MISC{Freeman2016-jt,
  title        = "Toward publishing reproducible computation with Binder",
  booktitle    = "eLife",
  author       = "Freeman, Jeremy and Osheroff, Andrew",
  abstract     = "Binder makes it easy to include an interactive version of
                  your analysis, with the supporting data and code, alongside a
                  published paper.",
  month        =  may,
  year         =  2016,
  url          = "https://elifesciences.org/labs/a7d53a88/toward-publishing-reproducible-computation-with-binder",
  howpublished = "\url{https://elifesciences.org/labs/a7d53a88/toward-publishing-reproducible-computation-with-binder}",
  note         = "Accessed: 2017-12-11"
}

@ARTICLE{Henderson2017-vg,
  title         = "Deep Reinforcement Learning that Matters",
  author        = "Henderson, Peter and Islam, Riashat and Bachman, Philip and
                   Pineau, Joelle and Precup, Doina and Meger, David",
  abstract      = "In recent years, significant progress has been made in
                   solving challenging problems across various domains using
                   deep reinforcement learning (RL). Reproducing existing work
                   and accurately judging the improvements offered by novel
                   methods is vital to sustaining this progress. Unfortunately,
                   reproducing results for state-of-the-art deep RL methods is
                   seldom straightforward. In particular, non-determinism in
                   standard benchmark environments, combined with variance
                   intrinsic to the methods, can make reported results tough to
                   interpret. Without significance metrics and tighter
                   standardization of experimental reporting, it is difficult
                   to determine whether improvements over the prior
                   state-of-the-art are meaningful. In this paper, we
                   investigate challenges posed by reproducibility, proper
                   experimental techniques, and reporting procedures. We
                   illustrate the variability in reported metrics and results
                   when comparing against common baselines and suggest
                   guidelines to make future results in deep RL more
                   reproducible. We aim to spur discussion about how to ensure
                   continued progress in the field by minimizing wasted effort
                   stemming from results that are non-reproducible and easily
                   misinterpreted.",
  month         =  sep,
  year          =  2017,
  url           = "http://arxiv.org/abs/1709.06560",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1709.06560"
}

@MISC{Project_Jupyter_Contributors_undated-jn,
  title        = "Binder (beta)",
  author       = "{Project Jupyter Contributors}",
  url          = "https://mybinder.org/",
  howpublished = "\url{https://mybinder.org/}",
  note         = "Accessed: 2017-12-11"
}

@MANUAL{Project_Jupyter_Contributors_undated-no,
  title    = "A Gallery of {JupyterHub} Deployments",
  author   = "{Project Jupyter Contributors}",
  url      = "http://jupyterhub.readthedocs.io/en/latest/gallery-jhub-deployments.html"
}

@MISC{Pineau2017-sb,
  title      = "Reproducibility in Deep Reinforcement Learning and Beyond",
  author     = "Pineau, Joelle",
  month      =  dec,
  year       =  2017,
  url        = "https://twitter.com/xtimv/status/938917013086380032",
  conference = "Deep Reinforcement Learning Symposium, NIPS 2017"
}

@MISC{Project_Jupyter_Contributors2017-zr,
  title        = "Introducing Binder 2.0 --- share your interactive research
                  environment",
  booktitle    = "eLife",
  author       = "{Project Jupyter Contributors}",
  abstract     = "The Project Jupyter team shares its reboot of Binder, the
                  tool that allows researchers to make their GitHub
                  repositories executable by others.",
  month        =  nov,
  year         =  2017,
  url          = "https://elifesciences.org/labs/8653a61d/introducing-binder-2-0-share-your-interactive-research-environment",
  howpublished = "\url{https://elifesciences.org/labs/8653a61d/introducing-binder-2-0-share-your-interactive-research-environment}",
  note         = "Accessed: 2017-12-11"
}
