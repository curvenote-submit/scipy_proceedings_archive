
@techreport{kim_state-space_1999,
	type = {{MIT} {Press} {Books}},
	title = {State-{Space} {Models} with {Regime} {Switching}: {Classical} and {Gibbs}-{Sampling} {Approaches} with {Applications}},
	shorttitle = {State-{Space} {Models} with {Regime} {Switching}},
	url = {http://ideas.repec.org/b/mtp/titles/0262112388.html},
	abstract = {Both state-space models and Markov switching models have been highly productive paths for empirical research in macroeconomics and finance. This book presents recent advances in econometric methods that make feasible the estimation of models that have both features. One approach, in the classical framework, approximates the likelihood function; the other, in the Bayesian framework, uses Gibbs-sampling to simulate posterior distributions from data. The authors present numerous applications of these approaches in detail: decomposition of time series into trend and cycle, a new index of coincident economic indicators, approaches to modeling monetary policy uncertainty, Friedman's "plucking" model of recessions, the detection of turning points in the business cycle and the question of whether booms and recessions are duration-dependent, state-space models with heteroskedastic disturbances, fads and crashes in financial markets, long-run real exchange rates, and mean reversion in asset returns.},
	urldate = {2013-05-03},
	institution = {The MIT Press},
	author = {Kim, Chang-Jin and Nelson, Charles R.},
	year = {1999},
}

@article{durbin_simple_2002,
	title = {A simple and efficient simulation smoother for state space time series analysis},
	volume = {89},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/content/89/3/603},
	doi = {10.1093/biomet/89.3.603},
	abstract = {A simulation smoother in state space time series analysis is a procedure for drawing samples from the conditional distribution of state or disturbance vectors given the observations. We present a new technique for this which is both simple and computationally efficient. The treatment includes models with diffuse initial conditions and regression effects. Computational comparisons are made with the previous standard method. Two applications are provided to illustrate the use of the simulation smoother for Gibbs sampling for Bayesian inference and importance sampling for classical inference.},
	language = {en},
	number = {3},
	urldate = {2014-02-07},
	journal = {Biometrika},
	author = {Durbin, J. and Koopman, S. J.},
	month = aug,
	year = {2002},
	pages = {603--616},
}

@book{durbin_time_2012,
	title = {Time {Series} {Analysis} by {State} {Space} {Methods}: {Second} {Edition}},
	isbn = {978-0-19-964117-8},
	shorttitle = {Time {Series} {Analysis} by {State} {Space} {Methods}},
	abstract = {This new edition updates Durbin \& Koopman's important text on the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbance terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. Additions to this second edition include the filtering of nonlinear and non-Gaussian series. Part I of the book obtains the mean and variance of the state, of a variable intended to measure the effect of an interaction and of regression coefficients, in terms of the observations. Part II extends the treatment to nonlinear and non-normal models. For these, analytical solutions are not available so methods are based on simulation.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Durbin, James and Koopman, Siem Jan},
	month = may,
	year = {2012},
}

@book{harvey_forecasting_1990,
	title = {Forecasting, {Structural} {Time} {Series} {Models} and the {Kalman} {Filter}},
	isbn = {978-0-521-40573-7},
	abstract = {This book provides a synthesis of concepts and materials that ordinarily appear separately in time series and econometrics literature, presenting a comprehensive review of both theoretical and applied concepts. Perhaps the most novel feature of the book is its use of Kalman filtering together with econometric and time series methodology. From a technical point of view, state space models and the Kalman filter play a key role in the statistical treatment of structural time series models. This technique was originally developed in control engineering but is becoming increasingly important in economics and operations research. The book is primarily concerned with modeling economic and social time series and with addressing the special problems that the treatment of such series pose.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Harvey, Andrew C.},
	year = {1990},
}

@book{west_bayesian_1999,
	address = {New York},
	edition = {2nd edition},
	title = {Bayesian {Forecasting} and {Dynamic} {Models}},
	isbn = {978-0-387-94725-9},
	language = {English},
	publisher = {Springer},
	author = {West, Mike and Harrison, Jeff},
	month = mar,
	year = {1999},
	note = {00000},
}

@article{kim_stochastic_1998,
	title = {Stochastic {Volatility}: {Likelihood} {Inference} and {Comparison} with {ARCH} {Models}},
	volume = {65},
	issn = {0034-6527, 1467-937X},
	shorttitle = {Stochastic {Volatility}},
	url = {http://restud.oxfordjournals.org/content/65/3/361},
	doi = {10.1111/1467-937X.00050},
	abstract = {In this paper, Markov chain Monte Carlo sampling methods are exploited to provide a unified, practical likelihood-based framework for the analysis of stochastic volatility models. A highly effective method is developed that samples all the unobserved volatilities at once using an approximating offset mixture model, followed by an importance reweighting procedure. This approach is compared with several alternative methods using real data. The paper also develops simulation-based methods for filtering, likelihood evaluation and model failure diagnostics. The issue of model choice using non-nested likelihood ratios and Bayes factors is also investigated. These methods are used to compare the fit of stochastic volatility and GARCH models. All the procedures are illustrated in detail.},
	language = {en},
	number = {3},
	urldate = {2016-11-22},
	journal = {The Review of Economic Studies},
	author = {Kim, Sangjoon and Shephard, Neil and Chib, Siddhartha},
	month = jul,
	year = {1998},
	note = {01855},
	pages = {361--393},
}

@article{stock_core_2016,
	title = {Core {Inflation} and {Trend} {Inflation}},
	volume = {98},
	issn = {0034-6535},
	url = {http://dx.doi.org/10.1162/REST_a_00608},
	doi = {10.1162/REST_a_00608},
	abstract = {This paper examines empirically whether the measurement of trend inflation can be improved by using disaggregated data on sectoral inflation to construct indexes akin to core inflation but with a time-varying distributed lags of weights, where the sectoral weight depends on the timevarying volatility and persistence of the sectoral inflation series and on the comovement among sectors. The modeling framework is a dynamic factor model with time-varying coefficients and stochastic volatility as in Del Negro and Otrok (2008), and is estimated using U.S. data on seventeen components of the personal consumption expenditure inflation index.},
	number = {4},
	urldate = {2016-11-22},
	journal = {Review of Economics and Statistics},
	author = {Stock, James H. and Watson, Mark W.},
	month = mar,
	year = {2016},
	note = {00000},
	pages = {770--784},
}

@book{hyndman_forecasting_2008,
	title = {Forecasting with {Exponential} {Smoothing}: {The} {State} {Space} {Approach}},
	isbn = {978-3-540-71918-2},
	shorttitle = {Forecasting with {Exponential} {Smoothing}},
	abstract = {Exponential smoothing methods have been around since the 1950s, and are still the most popular forecasting methods used in business and industry. However, a modeling framework incorporating stochastic models, likelihood calculation, prediction intervals and procedures for model selection, was not developed until recently. This book brings together all of the important new results on the state space framework for exponential smoothing. It will be of interest to people wanting to apply the methods in their own area of interest as well as for researchers wanting to take the ideas in new directions. Part 1 provides an introduction to exponential smoothing and the underlying models. The essential details are given in Part 2, which also provide links to the most important papers in the literature. More advanced topics are covered in Part 3, including the mathematical properties of the models and extensions of the models for specific problems. Applications to particular domains are discussed in Part 4.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Hyndman, Rob and Koehler, Anne B. and Ord, J. Keith and Snyder, Ralph D.},
	month = jun,
	year = {2008},
	note = {Google-Books-ID: GSyzox8Lu9YC},
}

@book{hyndman2018forecasting,
	title = {Forecasting: principles and practice},
	publisher = {OTexts},
	author = {Hyndman, Rob J and Athanasopoulos, George},
	year = {2018},
}

@article{kim_business_1998,
	title = {Business {Cycle} {Turning} {Points}, {A} {New} {Coincident} {Index}, and {Tests} of {Duration} {Dependence} {Based} on a {Dynamic} {Factor} {Model} {With} {Regime} {Switching}},
	volume = {80},
	issn = {0034-6535},
	url = {https://doi.org/10.1162/003465398557447},
	doi = {10.1162/003465398557447},
	abstract = {The synthesis of the dynamic factor model of Stock and Watson (1989) and the regime-switching model of Hamilton (1989) proposed by Diebold and Rudebusch (1996) potentially encompasses both features of the business cycle identified by Burns and Mitchell (1946): (1) comovement among economic variables through the cycle and (2) nonlinearity in its evolution. However, maximum-likelihood estimation has required approximation. Recent advances in multimove Gibbs sampling methodology open the way to approximation-free inference in such non-Gaussian, nonlinear models. This paper estimates the model for U.S. data and attempts to address three questions: Are both features of the business cycle empirically relevant? Might the implied new index of coincident indicators be a useful one in practice? Do the resulting estimates of regime switches show evidence of duration dependence? The answers to all three would appear to be yes.},
	number = {2},
	urldate = {2020-08-07},
	journal = {The Review of Economics and Statistics},
	author = {Kim, Chang-Jin and Nelson, Charles R.},
	month = may,
	year = {1998},
	note = {Publisher: MIT Press},
	pages = {188--201},
}

@article{chan_efficient_2009,
	title = {Efficient simulation and integrated likelihood estimation in state space models},
	volume = {1},
	issn = {2040-3607},
	url = {http://dx.doi.org/10.1504/IJMMNO.2009.030090},
	abstract = {We consider the problem of implementing simple and efficient Markov chain Monte Carlo (MCMC) estimation algorithms for state space models. A conceptually transparent derivation of the posterior distribution of the states is discussed, which also leads to an efficient simulation algorithm that is modular, scalable and widely applicable. We also discuss a simple approach for evaluating the integrated likelihood, defined as the density of the data given the parameters but marginal of the state vector. We show that this high-dimensional integral can be easily evaluated with minimal computational and conceptual difficulty. Two empirical applications in macroeconomics demonstrate that the methods are versatile and computationally undemanding. In one application, involving a time-varying parameter model, we show that the methods allow for efficient handling of large state vectors. In our second application, involving a dynamic factor model, we introduce a new blocking strategy which results in improved MCMC mixing at little cost. The results demonstrate that the framework is simple, flexible and efficient.},
	number = {1-2},
	urldate = {2022-05-17},
	journal = {International Journal of Mathematical Modelling and Numerical Optimisation},
	author = {Chan, Joshua C.C. and Jeliazkov, Ivan},
	month = jan,
	year = {2009},
	note = {Publisher: Inderscience Publishers},
	keywords = {Kalman filter, dynamic factor model, Markov chain Monte Carlo, banded matrix, Bayesian estimation, collapsed sampler, likelihood estimation, macroeconomics, MCMC estimation, simulation, state smoothing, state space models, time-varying parameter model},
	pages = {101--120},
}

@article{salvatier_probabilistic_2016,
	title = {Probabilistic programming in {Python} using {PyMC3}},
	volume = {2},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-55},
	doi = {10.7717/peerj-cs.55},
	abstract = {Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.},
	language = {en},
	urldate = {2022-05-24},
	journal = {PeerJ Computer Science},
	author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
	month = apr,
	year = {2016},
	note = {Publisher: PeerJ Inc.},
	pages = {e55},
}

@inproceedings{mckinney_time_2011,
	title = {Time {Series} {Analysis} in {Python} with statsmodels},
	doi = {10.25080/Majora-ebaa42b7-012},
	booktitle = {Proceedings of the 10th {Python} in {Science} {Conference}},
	author = {McKinney, Wes and Perktold, Josef and Seabold, Skipper},
	editor = {Walt, Stéfan van der and Millman, Jarrod},
	year = {2011},
	pages = {107 -- 113},
}

@inproceedings{seabold_statsmodels_2010,
	title = {Statsmodels: {Econometric} and {Statistical} {Modeling} with {Python}},
	doi = {10.25080/Majora-92bf1922-011},
	booktitle = {Proceedings of the 9th {Python} in {Science} {Conference}},
	author = {Seabold, Skipper and Perktold, Josef},
	editor = {Walt, Stéfan van der and Millman, Jarrod},
	year = {2010},
	pages = {92 -- 96},
}

@article{brodersen_inferring_2015,
	title = {Inferring causal impact using {Bayesian} structural time-series models},
	doi = {10.1214/14-aoas788},
	volume = {9},
	journal = {Annals of Applied Statistics},
	author = {Brodersen, Kay H. and Gallusser, Fabian and Koehler, Jim and Remy, Nicolas and Scott, Steven L.},
	year = {2015},
	pages = {247--274},
}

@techreport{taylor_forecasting_2017,
	title = {Forecasting at scale},
	url = {https://peerj.com/preprints/3190},
	abstract = {Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high quality forecasts — especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting “at scale” that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
	language = {en},
	number = {e3190v2},
	urldate = {2022-05-24},
	institution = {PeerJ Inc.},
	author = {Taylor, Sean J. and Letham, Benjamin},
	month = sep,
	year = {2017},
	doi = {10.7287/peerj.preprints.3190v2},
	note = {ISSN: 2167-9843},
}

@article{kumar_arviz_2019,
	title = {{ArviZ} a unified library for exploratory analysis of {Bayesian} models in {Python}},
	volume = {4},
	url = {https://doi.org/10.21105/joss.01143},
	doi = {10.21105/joss.01143},
	number = {33},
	journal = {Journal of Open Source Software},
	author = {Kumar, Ravin and Carroll, Colin and Hartikainen, Ari and Martin, Osvaldo},
	year = {2019},
	note = {Publisher: The Open Journal},
	pages = {1143},
}

@article{carpenter_stan_2017,
	title = {Stan : {A} {Probabilistic} {Programming} {Language}},
	volume = {76},
	issn = {1548-7660},
	shorttitle = {Stan},
	url = {https://www.osti.gov/pages/biblio/1430202-stan-probabilistic-programming-language},
	doi = {10.18637/jss.v076.i01},
	abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can also be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
	language = {English},
	number = {1},
	urldate = {2022-05-24},
	journal = {Journal of Statistical Software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	month = jan,
	year = {2017},
	note = {Institution: Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA (United States)},
}

@techreport{dillon_tensorflow_2017,
	title = {{TensorFlow} {Distributions}},
	url = {http://arxiv.org/abs/1711.10604},
	abstract = {The TensorFlow Distributions library implements a vision of probability theory adapted to the modern deep-learning paradigm of end-to-end differentiable computation. Building on two basic abstractions, it offers flexible building blocks for probabilistic computation. Distributions provide fast, numerically stable methods for generating samples and computing statistics, e.g., log density. Bijectors provide composable volume-tracking transformations with automatic caching. Together these enable modular construction of high dimensional distributions and transformations not possible with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible residual networks). They are the workhorse behind deep probabilistic programming systems like Edward and empower fast black-box inference in probabilistic models built on deep-network components. TensorFlow Distributions has proven an important part of the TensorFlow toolkit within Google and in the broader deep learning community.},
	number = {arXiv:1711.10604},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A.},
	month = nov,
	year = {2017},
	doi = {10.48550/arXiv.1711.10604},
	note = {arXiv:1711.10604 [cs, stat]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning},
}

@unpublished{fulton_estimating_2015,
	title = {Estimating time series models by state space methods in python: {Statsmodels}},
	author = {Fulton, Chad},
	year = {2015},
}
