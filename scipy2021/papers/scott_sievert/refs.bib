@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{bottou2018optimization,
  author = {Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  title = {Optimization methods for large-scale machine learning},
  journal = {SIAM Review},
  volume = {60},
  number = {},
  pages = {223–223},
  year = {2018},
  doi = {10.1137/16M1080173},
}

@misc{sievert2021improving,
  author = {Sievert, Scott},
  title = {Improving the convergence of SGD through adaptive batch sizes},
  journal = {arXiv preprint arXiv:1910.08222},
  volume = {},
  number = {},
  pages = {},
  year = {2020},
}

@Article{smith2017,
author = {Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc Vgra},
title = {Don't decay the learning rate, increase the batch size},
journal = {arXiv preprint arXiv:1711.00489},
volume = {},
number = {},
pages = {},
year = {2017},
}

@Article{goyal2017accurate,
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  title = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  journal = {arXiv preprint arXiv:1706.02677},
  volume = {},
  number = {},
  pages = {},
  year = {2017},
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={arXiv preprint arXiv:1912.01703},
  year={2019}
}

@inproceedings{zhou2018new,
 author = {Zhou, Pan and Yuan, Xiaotong and Feng, Jiashi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity},
 url = {https://proceedings.neurips.cc/paper/2018/file/67e103b0761e60683e83c559be18d40c-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{de2016big,
  author = {De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom},
  title = {{Big Batch SGD}: {Automated} Inference using Adaptive Batch Sizes},
  journal = {arXiv preprint arXiv:1610.05792},
  volume = {},
  number = {},
  pages = {},
  year = {2016},
}

@inproceedings{balles2016coupling,
  title={Coupling Adaptive Batch Sizes with Learning Rates},
  author={Balles, Lukas and Romero, Javier and Hennig, Philipp},
  booktitle={33rd Conference on Uncertainty in Artificial Intelligence (UAI 2017)},
  pages={675--684},
  year={2017},
  organization={Curran Associates, Inc.},
  url = {http://auai.org/uai2017/proceedings/papers/141.pdf},
}

@article{byrd2012,
  author = {Byrd, Richard H and Chin, Gillian M and Nocedal, Jorge and Wu, Yuchen},
  title = {Sample size selection in optimization methods for machine learning},
  journal = {Mathematical programming},
  volume = {134},
  number = {1},
  pages = {127–155},
  year = {2012},
  doi = {10.1007/s10107-012-0572-5},
}

@article{adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@article{adagrad,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121-2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@article{asgd,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM},
  doi = {10.1137/0330046},
}


@Article{zagoruyko2016b,
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  title = {Wide residual networks},
  journal = {arXiv preprint arXiv:1605.07146},
  volume = {},
  number = {},
  pages = {},
  year = {2016},
}

@Article{cifar10,
author = {Krizhevsky, Alex and Hinton, Geoffrey},
title = {Learning multiple layers of features from tiny images},
journal = {},
volume = {},
number = {},
pages = {},
year = {2009},
}

@inproceedings{rmsprop,
  title={A sufficient condition for convergences of adam and rmsprop},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11127--11135},
  year={2019},
  doi = {10.1109/cvpr.2019.01138},
}

@inproceedings{yin2018,
  title = {Gradient Diversity: a Key Ingredient for Scalable Distributed Learning},
  author = {Dong Yin and Ashwin Pananjady and Max Lam and Dimitris Papailiopoulos and Kannan Ramchandran and Peter Bartlett},
  booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = {1998--2007},
  year = {2018},
  editor = {Amos Storkey and Fernando Perez-Cruz},
  volume = {84},
  series = {Proceedings of Machine Learning Research},
  month = {09--11 Apr},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v84/yin18a/yin18a.pdf},
  url = { http://proceedings.mlr.press/v84/yin18a.html },
}

@article{khaled2020unified,
  title={Unified analysis of stochastic gradient methods for composite convex and smooth optimization},
  author={Khaled, Ahmed and Sebbouh, Othmane and Loizou, Nicolas and Gower, Robert M and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.11573},
  year={2020}
}

@inproceedings{gazagnadou2019optimal,
  title = {Optimal Mini-Batch and Step Sizes for {SAGA}},
  author = {Gazagnadou, Nidham and Gower, Robert and Salmon, Joseph},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages = {2142--2150},
  year = {2019},
  editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = {97},
  series = {Proceedings of Machine Learning Research},
  month = {09--15 Jun},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v97/gazagnadou19a/gazagnadou19a.pdf},
  url = { http://proceedings.mlr.press/v97/gazagnadou19a.html },
}

@article{perrone2019optimal,
  title={Optimal Mini-Batch Size Selection for Fast Gradient Descent},
  author={Perrone, Michael P and Khan, Haidar and Kim, Changhoan and Kyrillidis, Anastasios and Quinn, Jerry and Salapura, Valentina},
  journal={arXiv preprint arXiv:1911.06459},
  year={2019}
}

@inproceedings{zhang2019algorithmic,
 author = {Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger B},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model},
 url = {https://proceedings.neurips.cc/paper/2019/file/e0eacd983971634327ae1819ea8b6214-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{johnson2020adascale,
  title = 	 {{A}da{S}cale {SGD}: A User-Friendly Algorithm for Distributed Training},
  author =       {Johnson, Tyler and Agrawal, Pulkit and Gu, Haijie and Guestrin, Carlos},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4911--4920},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/johnson20a/johnson20a.pdf},
  url = 	 {http://proceedings.mlr.press/v119/johnson20a.html},
}


@inproceedings{alistarh2016qsgd,
 author = {Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding},
 url = {https://proceedings.neurips.cc/paper/2017/file/6c340f25839e6acdc73414517203f5f0-Paper.pdf},
 volume = {30},
 year = {2017}
}


@inproceedings{wang2018atomo,
 author = {Wang, Hongyi and Sievert, Scott and Liu, Shengchao and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ATOMO: Communication-efficient Learning via Atomic Sparsification},
 url = {https://proceedings.neurips.cc/paper/2018/file/33b3214d792caf311e1f00fd22b392c5-Paper.pdf},
 volume = {31},
 year = {2018}
}


@inproceedings{grubic2018synchronous,
  title={Synchronous multi-gpu deep learning with low-precision communication: An experimental study},
  author={Grubic, Demjan and Tam, Leo K and Alistarh, Dan and Zhang, Ce},
  booktitle={Proceedings of the 21st International Conference on Extending Database Technology},
  pages={145--156},
  year={2018},
  organization={OpenProceedings},
  doi = {10.3929/ethz-b-000319485},
}

@article{sergeev2018horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}


@article{li2020pytorch,
  author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
  title = {PyTorch Distributed: Experiences on Accelerating Data Parallel Training},
  year = {2020},
  issue_date = {August 2020},
  publisher = {VLDB Endowment},
  volume = {13},
  number = {12},
  issn = {2150-8097},
  url = {https://doi.org/10.14778/3415478.3415530},
  doi = {10.14778/3415478.3415530},
  journal = {Proc. VLDB Endow.},
  month = aug,
  pages = {3005–3018},
  numpages = {14}
}

@article{dekel2012optimal,
  author  = {Ofer Dekel and Ran Gilad-Bachrach and Ohad Shamir and Lin Xiao},
  title   = {Optimal Distributed Online Prediction Using Mini-Batches},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {6},
  pages   = {165-202},
  url     = {http://jmlr.org/papers/v13/dekel12a.html}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}


@inproceedings{abadi2016,
  author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen
      and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat
          and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh
          Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and
          Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden
          and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  title = {TensorFlow: A System for Large-Scale Machine Learning},
  booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
  year = {2016},
  isbn = {978-1-931971-33-1},
  address = {Savannah, GA},
  pages = {265--283},
  url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
  publisher = {{USENIX} Association},
  month = nov,
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@article{jia2018,
author = {Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei},
title = {Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes},
journal = {arXiv preprint arXiv:1807.11205},
volume = {},
number = {},
pages = {},
year = {2018},
}

@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@article{golmant2018computational,
  title={On the computational inefficiency of large batch sizes for stochastic gradient descent},
  author={Golmant, Noah and Vemuri, Nikita and Yao, Zhewei and Feinberg, Vladimir and Gholami, Amir and Rothauge, Kai and Mahoney, Michael W and Gonzalez, Joseph},
  journal={arXiv preprint arXiv:1811.12941},
  year={2018}
}

@inproceedings{li2014scaling,
  author = {Mu Li and David G. Andersen and Jun Woo Park and Alexander J. Smola and Amr Ahmed and Vanja Josifovski and James Long and Eugene J. Shekita and Bor-Yiing Su},
  title = {Scaling Distributed Machine Learning with the Parameter Server},
  booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
  year = {2014},
  isbn = { 978-1-931971-16-4},
  address = {Broomfield, CO},
  pages = {583--598},
  url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu},
  publisher = {{USENIX} Association},
  month = oct,
}

@inproceedings{qi2017paleo,
  title={Paleo: A Performance Model for Deep Neural Networks.},
  author={Qi, Hang and Sparks, Evan R and Talwalkar, Ameet},
  booktitle={ICLR (Poster)},
  year={2017},
  url={https://talwalkarlab.github.io/paleo/},
}


@book{nesterov2013a,
  title={Introductory lectures on convex programming volume i: Basic course},
  author={Nesterov, Yurii},
  journal={Lecture notes},
  volume={3},
  number={4},
  pages={5},
  year={1998}
}

@inproceedings{recht2011hogwild,
 author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf},
 volume = {24},
 year = {2011}
}



@Article{zhang2016hogwild++,
    author={Zhang, Huan and Hsieh, Cho-Jui and Akella, Venkatesh},
    booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)},
    title={HogWild++: A New Mechanism for Decentralized Asynchronous Stochastic Gradient Descent},
    year={2016},
    volume={},
    number={},
    pages={629-638},
    doi={10.1109/ICDM.2016.0074}
}

@Article{dean2012large,
 author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Large Scale Distributed Deep Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf},
 volume = {25},
 year = {2012}
}




@article{karimi2016linear,
author = {Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
title = {Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition},
journal = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
volume = {},
number = {},
pages = {795–795},
year = {2016},
doi = {10.1007/978-3-319-46128-1_50},
}

@inproceedings{recht2019imagenet,
    title = {Do {I}mage{N}et Classifiers Generalize to {I}mage{N}et?},
    author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
    booktitle = {Proceedings of the 36th International Conference on Machine Learning},
    pages = {5389--5400},
    year = {2019},
    editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
    volume = {97},
    series = {Proceedings of Machine Learning Research},
    month = {09--15 Jun},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v97/recht19a/recht19a.pdf},
    url = { http://proceedings.mlr.press/v97/recht19a.html },
}
