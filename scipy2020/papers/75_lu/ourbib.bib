@misc{Hale2018,
      title={Smarter Ways to Encode Categorical Data for Machine Learning: Exploring Category Encoders},
      url={https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159},
      journal={Towards Data Science},
      author={Hale, Jeff},
      year={2018},
      month={Sep}
}
@unpublished{kainan,
      author = {Kainen, Paul},
      year = {1992},
      note={Unpublished report, Washington DC: Industrial Math},
      title = {Orthogonal dimension and tolerance}
}
@misc{sphere,
      title={Spherical Codes:
             Nice arrangements of points on a sphere in various dimensions},
      author={N. J. A. {Sloane} and R. H. {Hardin} and W. D. {Smith}},
      url={http://neilsloane.com/packings/},
      year={2020},
      note = "[Online; accessed 15-May-2020]"
}
@INPROCEEDINGS{kurkova,
               author={V. {Kůrková} and P. C. {Kainen}},
               booktitle={Proceedings of International Conference on Neural Networks (ICNN'96)},
               title={A geometric method to obtain error-correcting classification by neural networks with fewer hidden units},
               year={1996},
               volume={2}, 
               number={}, 
               pages={1227-1232 vol.2},
               doi={10.1109/ICNN.1996.549073}
}

@Inbook{Kainen2020,
        author="Kainen, Paul C.
        and K{\r{u}}rkov{\'a}, V{\v{e}}ra",
        editor="Kosheleva, Olga
        and Shary, Sergey P.
        and Xiang, Gang
        and Zapatrin, Roman",
        title="Quasiorthogonal Dimension",
        bookTitle="Beyond Traditional Probabilistic Data Processing Techniques: Interval, Fuzzy etc. Methods and Their Applications",
        year="2020",
        publisher="Springer International Publishing",
        address="Cham",
        pages="615--629",
        abstract="An interval approach to the concept of dimension is presented. The concept of quasiorthogonal dimension is obtained by relaxing exact orthogonality so that angular distances between unit vectors are constrained to a fixed closed symmetric interval about {\$}{\$}{\backslash}pi /2{\$}{\$}. An exponential number of such quasiorthogonal vectors exist as the Euclidean dimension increases. Lower bounds on quasiorthogonal dimension are proven using geometry of high-dimensional spaces and a separate argument is given utilizing graph theory. Related notions are reviewed.",
        isbn="978-3-030-31041-7",
        doi="10.1007/978-3-030-31041-7_35"
}


@misc{scikit,
      title={Category Encoders},
      url={http://contrib.scikit-learn.org/category_encoders/},
      author={Will McGinnis},
      year={2016}
}

@misc{tensorflow,
      title={TensorFlow 2 Quickstart for Beginners},
      url={https://www.tensorflow.org/tutorials/quickstart/beginner/},
      author={Tensorflow},
      year={2019}
}

@misc{hash,
      title={Don’t be tricked by the Hashing Trick}, 
      url={https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087}, 
      journal={Booking.com Data Science}, author={Bernardi, Lucas}, year={2018}, month={Jan}
}
                                                                                      
@techreport{Gautam2013ANA,
            title={A Novel Approach to the Spherical Codes Problem},
            author={Simanta Gautam and Dmitry Vaintrob},
            institution= {Massachusetts Institute of Technology},
            year={2012}
}
@misc{weisstein,
      author = "{Eric W. Weisstein}",
      title = "Spherical Code",
      booktitle = "From MathWorld- A Wolfram Resource",
      year = "2020",
      url = "https://mathworld.wolfram.com/SphericalCode.html",
      note = "[Online; accessed 18-May-2020]"
}

@InProceedings{gondara,
author="Gondara, Lovedeep
and Wang, Ke",
editor="Phung, Dinh
and Tseng, Vincent S.
and Webb, Geoffrey I.
and Ho, Bao
and Ganji, Mohadeseh
and Rashidi, Lida",
title="MIDA: Multiple Imputation Using Denoising Autoencoders",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="260--272",
abstract="Missing data is a significant problem impacting all domains. State-of-the-art framework for minimizing missing data bias is multiple imputation, for which the\
 choice of an imputation model remains nontrivial. We propose a multiple imputation model based on overcomplete deep denoising autoencoders. Our proposed model is capab\
le of handling different data types, missingness patterns, missingness proportions and distributions. Evaluation on several real life datasets show our proposed model s\
ignificantly outperforms current state-of-the-art methods under varying conditions while simultaneously improving end of the line analytics.",
isbn="978-3-319-93040-4",
doi = {10.1007/978-3-319-93040-4_21}
}
@inproceedings{lu,
    author={Haw-minn Lu and Giancarlo Perrone and José Unpingco},
    editor    = {Petra Perner},
    title={Multiple Imputation with Denoising Autoencoder using Metamorphic Truth and Imputation Feedback},
    booktitle = {Machine Learning and Data Mining in Pattern Recognition, 16th International Conference on Machine Learning and Data Mining, {MLDM} 2020, Amsterdam, The Netherlands, July 20-21, 2020, Proceedings},
    pages     = {197--208},
    publisher = {ibai publishing},
    year      = {2019},
    isbn      = {978-3-942952-75-0},
    url = {http://www.ibai-publishing.org/html/proceeding2020.php}
}
@article{mnist,
    author = {LeCun, Yann and Cortes, Corinna},
    description = {Nur für Referenzzwecke verwendet (MNIST)},
    groups = {public},
    howpublished = {http://yann.lecun.com/exdb/mnist/},
    title = {{MNIST} handwritten digit database},
    url = {http://yann.lecun.com/exdb/mnist/},
    year = 2010
                          }


@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@book{lindner2017design,
  title={Design theory},
  author={Lindner, Charles C and Rodger, Christopher A},
  year={2017},
  publisher={CRC press},
  doi = {10.1201/9781315107233},
}
@InProceedings{wordembeddings,
author="Guti{\'e}rrez, Luis
and Keith, Brian",
editor="Mejia, Jezreel
and Mu{\~{n}}oz, Mirna
and Rocha, {\'A}lvaro
and Pe{\~{n}}a, Adriana
and P{\'e}rez-Cisneros, Marco",
title="A Systematic Literature Review on Word Embeddings",
booktitle="Trends and Applications in Software Engineering",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="132--141",
abstract="This article presents a systematic literature review on word embeddings within the field of natural language processing and text processing. A search and classification of 140 articles on proposals of word embeddings or their application was carried out from three different sources. Word embeddings have been widely adopted with satisfactory results in natural language processing tasks in general and other domains with good results. In this paper, we report the hegemony of word embeddings based on neural models over those generated by matrix factorization (i.e., variants of word2vec). Finally, despite the good performance of word embeddings, some drawbacks and their respective solution proposals are identified, such as the lack of interpretability of the real values that make up the embedded vectors.",
isbn="978-3-030-01171-0",
doi = {10.1007/978-3-030-01171-0_12},
}

@misc{mukhtar,
      title={High Dimensional Data: Breaking the Curse of Dimensionality with Python},
      url={https://blog.datasciencedojo.com/curse-of-dimensionality-python/},
      journal={Data Science Dojo},
      author={Tooba Mukhtar},
      year={2019},
      month={Apr}
}
